{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus semantic & syntactic analysis\n",
    "In this notebooks, I'll explore the following analyses of corpus semantics and syntactics:\n",
    "1. **Vocabulary analysis.** Look for out-of-vocabulary words in each pairwise comparison of corpora. The more out-of-vocabulary words for a given pair, the less similar they are to one another. Expect that SciERC and PICKLE will have more overlapping vocabulary.\n",
    "2. **Token analysis.** Using a word2vec model, embed the tokens from each corpus, and see how they cluster in the vector space. Expect that PICKLE and SciERC tokens will cluster together in space.\n",
    "3. **Document analysis.** Using some kind of document embedding (doc2vec, a BERT based embedding, etc), perform the same style of analysis as for the tokens. Expect PICKLE and SciERC documents to cluster together more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import jsonlines\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importing my own Dataset class\n",
    "import sys\n",
    "sys.path.append('../models/corpus_comparison/')\n",
    "from dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vocabulary analysis\n",
    "For this analysis, we'll use the SciERC, GENIA, and PICKLE corpora to get the sets of tokens that make up the three corpora. We'll then look for out-of-vocabulary words in each pairwise comparison of corpora. Based on the pre-trained model results, in which SciERC outperformed GENIA on the PICKLE corpus, we expect to see that there are more out-of-vocabulary words in the GENIA-PICKLE comparison than in the SciERC-PICKLE comparison.\n",
    "### Getting our corpus vocabularies\n",
    "First, we have to get the GENIA and SciERC datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading SciERC and GENIA data\n",
    "We'll use the code provided in the [DyGIE++](https://github.com/dwadden/dygiepp) repo for this purpose, running the following commands from the root of the `dygiepp` repo:\n",
    "\n",
    "```\n",
    "bash ./scripts/data/get_scierc.sh\n",
    "```\n",
    "\n",
    "```\n",
    "bash ./scripts/data/get_genia.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing vocabularies\n",
    "Now that we have all three datasets, we can compare them. This is done with the script `out_of_vocab_comparison.py`, found in the `models/corpus_comparison` directory.\n",
    "\n",
    "<br>\n",
    "\n",
    "*A summary of the comparisons:* For each pair of datasets, a total of 6 metrics were calculated. For each of unigrams, bigrams, and trigrams, the datasets were compared in both directions, and the number of out-of-vocabulary words for that comparison were computed. In order to be able to compare the differences between PICKLE/GENIA and PICKLE/SciERC, the comparisons were normalized by the number of words of that n-gram length in the comparison corpus, making the final calculation equivalent to:\n",
    "\n",
    "```len(set(ngrams in PICKLE) - set(ngrams in GENIA/SciERC)) / len(ngrams in GENIA/SciERC)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commands used to generate comparisons\n",
    "The relative paths here are specific to my local filesystem/data directory structure, which doesn't come along with this repository. These commands were run form the `models/corpus_comparison/` directory.\n",
    "\n",
    "To compare GENIA to PICKLE:\n",
    "```\n",
    "python out_of_vocab_comparison.py pickle ../../data/straying_off_topic_data/unified_annotations_processed/jsonl_files/FINAL_FOR_EVALUATION_ent_and_rel_all_gold_std_abstracts_08Nov2022.jsonl genia ../../../dygiepp/data/genia/processed-data/json-coref-ident-only/train.json  ../../data/straying_off_topic_data/corpus_comparison/ pickle_vs_genia_normalized -v\n",
    "```\n",
    "\n",
    "To compare SciERC to PICKLE:\n",
    "```\n",
    "python out_of_vocab_comparison.py pickle ../../data/straying_off_topic_data/unified_annotations_processed/jsonl_files/FINAL_FOR_EVALUATION_ent_and_rel_all_gold_std_abstracts_08Nov2022.jsonl scierc ../../../dygiepp/data/scierc/processed_data/json/train.json  ../../data/straying_off_topic_data/corpus_comparison/ pickle_vs_scierc_normalized -v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at comparison outputs\n",
    "Now, lets read in the data and see what the situation is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "genia_vs_pickle_path = '../data/straying_off_topic_data/corpus_comparison/pickle_vs_genia_normalized_oov_comparison.jsonl'\n",
    "scierc_vs_pickle_path = '../data/straying_off_topic_data/corpus_comparison/pickle_vs_scierc_normalized_oov_comparison.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "genia_vs_pickle = []\n",
    "with jsonlines.open(genia_vs_pickle_path) as reader:\n",
    "    for obj in reader:\n",
    "        genia_vs_pickle.append(obj)\n",
    "        \n",
    "scierc_vs_pickle = []\n",
    "with jsonlines.open(scierc_vs_pickle_path) as reader:\n",
    "    for obj in reader:\n",
    "        scierc_vs_pickle.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize the situation, let's make a bar plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pickle_labels = ['SciERC', 'GENIA']\n",
    "unigrams = [scierc_vs_pickle[0]['unigrams_oov_pickle_frac'], genia_vs_pickle[0]['unigrams_oov_pickle_frac']]\n",
    "bigrams = [scierc_vs_pickle[0]['bigrams_oov_pickle_frac'], genia_vs_pickle[0]['bigrams_oov_pickle_frac']]\n",
    "trigrams = [scierc_vs_pickle[0]['trigrams_oov_pickle_frac'], genia_vs_pickle[0]['trigrams_oov_pickle_frac']]\n",
    "\n",
    "x = np.arange(len(pickle_labels))\n",
    "x = 0.5*x\n",
    "width = 0.1\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "plt.bar(x - width, unigrams, width, color='tab:purple', label='Unigrams')\n",
    "plt.bar(x, bigrams, width, color='tab:grey', label='Bigrams')\n",
    "plt.bar(x + width, trigrams, width, color='tab:brown', label='Trigrams')\n",
    "\n",
    "plt.ylabel('Out-of-vocabulary fraction for PICKLE')\n",
    "plt.xticks(x, pickle_labels)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.savefig('../data/straying_off_topic_data/corpus_comparison/figures/oov_plot.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all three categories, the amount of PICKLE's vocabulary that is out of the vocabulary of the other corpus is much greater for SciERC than for GENIA. This is the opposite of what we expected, since SciERC performs better for entity recognition on the PICKLE corpus than GENIA does. However, it's possible that the semantics of the words in SciERC are more similar to those in PICKLE than in GENIA, which could account for the phenomenon. That's what we'll look at next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Token Analysis\n",
    "Now, lets generate word vectors for the unigram, bigram, and trigram tokens in the three corpora, and compare their semantics by looking at how those vectors cluster.\n",
    "\n",
    "<br>\n",
    "\n",
    "We'll use the script `models/corpus_comparison/train_word2vec.py` to generate these word vectors, and read them in here. This script uses all the uni-, bi-, and trigrams from all three corpora to train a new Word2Vec model using the [gensim implementation](https://radimrehurek.com/gensim/models/word2vec.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to generate the word vectors\n",
    "The following command was used to generate the word vectors:\n",
    "```\n",
    "python train_word2vec.py -corpora ../../data/straying_off_topic_data/unified_annotations_processed/jsonl_files/FINAL_ENTITIES_all_gold_standard_abstracts_GOLD_STD_03Mar2022.jsonl ../../../dygiepp/data/scierc/processed_data/json/train.json ../../../dygiepp/data/genia/processed-data/json-coref-ident-only/train.json -out_loc ../../data/straying_off_topic_data/corpus_comparison/ -out_prefix pickle_scierc_genia_combined -v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in and plotting the word vectors\n",
    "Now we want to take a look at the differences between the vectors from the different corpora. In order to do this, we're going to read them in, sort them to the corpus from which they came, and then using PCA to generate 2-D versions of the vectors that we can visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word vectors\n",
    "word2vecs = '../data/straying_off_topic_data/corpus_comparison/pickle_scierc_genia_combined_word2vec_skipgram.wordvectors'\n",
    "vecs = KeyedVectors.load(word2vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datasets\n",
    "def read_dataset(path, name):\n",
    "    \"\"\"\n",
    "    Read in a dataset to a Dataset object.\n",
    "    \"\"\"\n",
    "    objs = []\n",
    "    with jsonlines.open(path) as reader:\n",
    "        for obj in reader:\n",
    "            objs.append(obj)\n",
    "            \n",
    "    dset = Dataset(name, objs)\n",
    "    \n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scierc_path = '../../dygiepp/data/scierc/processed_data/json/train.json'\n",
    "genia_path = '../../dygiepp/data/genia/processed-data/json-coref-ident-only/train.json'\n",
    "pickle_path = '../data/straying_off_topic_data/unified_annotations_processed/jsonl_files/FINAL_ENTITIES_all_gold_standard_abstracts_GOLD_STD_03Mar2022.jsonl'\n",
    "\n",
    "scierc = read_dataset(scierc_path, 'scierc')\n",
    "genia = read_dataset(genia_path, 'genia')\n",
    "pickle = read_dataset(pickle_path, 'pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort vectors\n",
    "# Heads up that this is slow\n",
    "sep_vecs = {}\n",
    "for dset in [scierc, genia, pickle]:\n",
    "    _ = dset.get_dataset_vocab()\n",
    "    dset_comb_vocab = list(dset.vocab['unigrams']) + list(dset.vocab['bigrams']) + list(dset.vocab['trigrams'])\n",
    "    dset_vecs = {k:vecs[k] for k in vecs.index_to_key if k in dset_comb_vocab}\n",
    "    sep_vecs[dset.get_dataset_name()] = dset_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legend solution in below code is from [this stackoverflow answer](https://stackoverflow.com/a/31311862/13340814)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do PCA to get plottable shaped vectors\n",
    "# Fit on all the vectors, then transform each individually to keep the sorted structure\n",
    "\n",
    "# Make input array\n",
    "X = []\n",
    "for k in vecs.index_to_key:\n",
    "    X.append(vecs[k])\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "# Transform the vectors\n",
    "x, y, group = [], [], []\n",
    "for dset in sep_vecs:\n",
    "    for word in sep_vecs[dset]:\n",
    "        word_vec = sep_vecs[dset][word].reshape(1, -1)\n",
    "        transformed = pca.transform(word_vec).flatten()\n",
    "        x.append(transformed[0])\n",
    "        y.append(transformed[1])\n",
    "        group.append(dset)\n",
    "        \n",
    "# Plot\n",
    "color_dict = {'scierc': 'blue', 'genia':'orange', 'pickle':'green'}\n",
    "colors = [color_dict[i] for i in group]\n",
    "scatter = plt.scatter(x=x, y=y, c=colors, alpha = 0.5, s=4)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "markers = [plt.Line2D([0,0],[0,0],color=color, marker='o', linestyle='') for color in color_dict.values()]\n",
    "plt.legend(markers, color_dict.keys(), numpoints=1)\n",
    "\n",
    "plt.savefig('../data/straying_off_topic_data/corpus_comparison/figures/full_corpus_wordvec_compare.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further exploration of word vectors with clustering\n",
    "Generally speaking, it looks like all three corpora form two clusters. There are no telling visual differences between the three corpora here, but it might be helpful to see what typical words from those two clusters look like for each of the corpora. In orer to do this, we'll use K-Means clustering with k=2 for each of the three corpoora. We can then draw random words from each cluster, as well as compute the distance between the centroids of each cluster, for all three corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the data\n",
    "cluster_prep = {}\n",
    "for dset in sep_vecs:\n",
    "    vec_df = pd.DataFrame.from_dict(sep_vecs[dset], orient='index')\n",
    "    cluster_prep[dset] = vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run clustering alg to get cluster labels for all rows in df\n",
    "def get_clusters(cluster_prep, k=2):\n",
    "    \"\"\"\n",
    "    Get cluster labels and centroid for each dataset.\n",
    "    \n",
    "    parameters:\n",
    "        cluster_prep, dict: keys are dset names, values are arrays of vectors\n",
    "        k, int: number of clusters\n",
    "    \"\"\"\n",
    "    clusters = {}\n",
    "    centroids = {}\n",
    "    for dset in cluster_prep:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0).fit(cluster_prep[dset])\n",
    "        preds = kmeans.predict(cluster_prep[dset])\n",
    "        pred_df = pd.DataFrame({'word': cluster_prep[dset].index, 'cluster': preds})\n",
    "        clusters[dset] = pred_df\n",
    "        centroids[dset] = kmeans.cluster_centers_\n",
    "        \n",
    "    return clusters, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_2, centroids_2 = get_clusters(cluster_prep, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our clusters, let's make three separate plots, one for each dataset, where the two clusters are colored differently, with their centroids marked with a large red dot. This means we have to do PCA again to get the plottable forms. We still want to fit the PCA on all three datasets together, so the resulting plots are comparable, but we then want to separate out the three datasets into different plots, and use the cluster labels to color them instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do PCA to get plottable shaped vectors\n",
    "# Fit on all the vectors, then transform each individually to keep the sorted structure\n",
    "\n",
    "def plot_clusters(vecs, cluster_prep, clusters, centroids, color_dict):\n",
    "    \"\"\"\n",
    "    Plot clusters.\n",
    "    \n",
    "    parameters:\n",
    "        vecs, KeyedVector obj: original vector object\n",
    "        cluster_prep, dict: keys are dset names, values are arrays of vectors\n",
    "        clusters, dict: keys are dset names, values are df with word as idx and cluster number as column\n",
    "        centroids, dict: keys are dset names, values are arrays with centroids\n",
    "        color_dict, dict: keys are cluster labels, values are colors\n",
    "    \"\"\"\n",
    "    # Make input array\n",
    "    X = []\n",
    "    for k in vecs.index_to_key:\n",
    "        X.append(vecs[k])\n",
    "\n",
    "    # Fit PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X)\n",
    "\n",
    "    # Make fig\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=1, sharex=True, sharey=True, figsize=(4, 9))\n",
    "    fig.supxlabel('PC1')\n",
    "    fig.supylabel('PC2')\n",
    "    plt.subplots_adjust(bottom=0.1, left=0.2)\n",
    "    \n",
    "    # Transform the vectors and plot\n",
    "    for dset, ax in zip(cluster_prep.keys(), axs):\n",
    "        # Word vectors\n",
    "        x, y, cluster = [], [], []\n",
    "        for i in range(cluster_prep[dset].shape[0]):\n",
    "            word_vec = cluster_prep[dset].iloc[i].to_numpy().reshape(1, -1)\n",
    "            transformed = pca.transform(word_vec).flatten()\n",
    "            x.append(transformed[0])\n",
    "            y.append(transformed[1])\n",
    "            cluster.append(clusters[dset].iloc[i].values[1])\n",
    "\n",
    "        # Centroids\n",
    "        centroid_arr = centroids[dset]\n",
    "        x_centr, y_centr = [], []\n",
    "        for row in centroid_arr:\n",
    "            sample = row.reshape(1, -1)\n",
    "            transformed = pca.transform(sample).flatten()\n",
    "            x_centr.append(transformed[0])\n",
    "            y_centr.append(transformed[1])\n",
    "\n",
    "        # Plot\n",
    "        colors = [color_dict[i] for i in cluster]\n",
    "        ax.scatter(x=x, y=y, c=colors, alpha = 0.5, s=4)\n",
    "        ax.scatter(x=x_centr, y=y_centr, c='red', s=20)\n",
    "        ax.set_title(dset)\n",
    "\n",
    "    plt.savefig(f'../data/straying_off_topic_data/corpus_comparison/figures/k{len(color_dict.keys())}_clusters_sep_dsets.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 2\n",
    "color_dict_2 = {0: 'blue', 1:'orange'}\n",
    "plot_clusters(vecs, cluster_prep, clusters_2, centroids_2, color_dict_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots actually make it look like there are 3 clusters, and k=2 isn't the correct choice.  The yellow cluster, in all 3 datasets, is clearly two clusters. Lets try that again, but with k=3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 3\n",
    "clusters_3, centroids_3 = get_clusters(cluster_prep, k=3)\n",
    "color_dict_3 = {0: 'blue', 1:'orange', 2:'cyan'}\n",
    "plot_clusters(vecs, cluster_prep, clusters_3, centroids_3, color_dict_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, PICKLE clusters differently than the other two corpora, looking more like it should maybe have 4 clusters instead of 3. However, I am inclined to believe that this is a consequence of the fact that PICKLE is so much smaller than the other two corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Look at random words from each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordvectors for unique words only\n",
    "Another approach to dig into what's going on here is to make the same plots as above (all three datsets together, each of the three datasets but clustered), but only using the words that each corpus contains uniquely of the other two. For example, what words are there in PICKLE that don't appear in either SciERC or GENIA? I would expect that the three corpora would form separate clusters, but with the semantic (euclidian) distance between the clusters for SciERC and PICKLE being less than the distance between GENIA and PICKLE. This rests on the assumption that a model will perform better on a new domain if the out-of-vocabulary words in the new domain are close to its own unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use set difference to get unique words form sep_vecs\n",
    "unique_sep_vecs = {}\n",
    "for dset in sep_vecs:\n",
    "    other_dset_names = [d for d in sep_vecs if dset != d]\n",
    "    other_dset_set = set(list(sep_vecs[other_dset_names[0]].keys()) + list(sep_vecs[other_dset_names[1]].keys()))\n",
    "    dset_set = set(list(sep_vecs[dset].keys()))\n",
    "    diff = dset_set.difference(other_dset_set)\n",
    "    un_vecs = {w:v for w, v in sep_vecs[dset].items() if w in diff}\n",
    "    unique_sep_vecs[dset] = un_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do PCA to get plottable shaped vectors\n",
    "# Fit on all the vectors, then transform each individually to keep the sorted structure\n",
    "\n",
    "# Make input array\n",
    "X = []\n",
    "for k in vecs.index_to_key:\n",
    "    X.append(vecs[k])\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "# Transform the vectors\n",
    "x, y, group = [], [], []\n",
    "for dset in unique_sep_vecs:\n",
    "    for word in unique_sep_vecs[dset]:\n",
    "        word_vec = unique_sep_vecs[dset][word].reshape(1, -1)\n",
    "        transformed = pca.transform(word_vec).flatten()\n",
    "        x.append(transformed[0])\n",
    "        y.append(transformed[1])\n",
    "        group.append(dset)\n",
    "        \n",
    "# Plot\n",
    "color_dict = {'scierc': 'blue', 'genia':'orange', 'pickle':'green'}\n",
    "colors = [color_dict[i] for i in group]\n",
    "scatter = plt.scatter(x=x, y=y, c=colors, alpha = 0.5, s=4)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "markers = [plt.Line2D([0,0],[0,0],color=color, marker='o', linestyle='') for color in color_dict.values()]\n",
    "plt.legend(markers, color_dict.keys(), numpoints=1)\n",
    "\n",
    "plt.savefig('../data/straying_off_topic_data/corpus_comparison/figures/unique_words_scatter.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst that doesn't correspond to my expectation, I do think that it looks like the PICKLE and SciERC unique words more frequently fall near each other in space, and that there are more \"distracting\" words in the GENIA unique words set. To test this, let's look at [some distance metrics](https://online.stat.psu.edu/stat505/lesson/14/14.4) used to compute the distance between clusters (we'll look at single, complete and average linkage). Here, we'll consider each dataset's unique word vectors to be a cluster, as those are the groupings we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dfs to get differences\n",
    "unique_dfs = {}\n",
    "for dset in unique_sep_vecs:\n",
    "    df = pd.DataFrame.from_dict(unique_sep_vecs[dset], orient='index')\n",
    "    unique_dfs[dset] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate raw differences\n",
    "distance_matrices = {}\n",
    "for pair in [('scierc', 'genia'), ('scierc', 'pickle'), ('genia', 'pickle')]:\n",
    "    dist = euclidean_distances(unique_dfs[pair[0]], unique_dfs[pair[1]])\n",
    "    distance_matrices[f'{pair[0]}_{pair[1]}'] = dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each matrix has unique values on both the x and y axes, so every comparison (value in the matrix) represents a unique pair. We can now get the max, min and average distance for each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_metrics = {}\n",
    "for pair in distance_matrices:\n",
    "    min_dist = distance_matrices[pair].min() # Single linkage\n",
    "    max_dist = distance_matrices[pair].max() # Complete linkage\n",
    "    mean_dist = distance_matrices[pair].mean() # Average linkage\n",
    "    pair_metrics[pair] = (min_dist, max_dist, mean_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a bar plot to easily visualize these distances for the comparisons to PICKLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_labels = ['SciERC', 'GENIA']\n",
    "single = [pair_metrics['scierc_pickle'][0], pair_metrics['genia_pickle'][0]]\n",
    "complete = [pair_metrics['scierc_pickle'][1], pair_metrics['genia_pickle'][1]]\n",
    "average = [pair_metrics['scierc_pickle'][2], pair_metrics['genia_pickle'][2]]\n",
    "\n",
    "x = np.arange(len(pair_labels))\n",
    "x = 0.5*x\n",
    "width = 0.1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, single, width, color='tab:pink', label='Single Linkage')\n",
    "rects2 = ax.bar(x, complete, width, color='tab:olive', label='Complete Linkage')\n",
    "rects3 = ax.bar(x + width, average, width, color='tab:cyan', label='Average Linkage')\n",
    "\n",
    "ax.set_ylabel('Linkage Distance')\n",
    "ax.set_xticks(x, pair_labels)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('../data/straying_off_topic_data/corpus_comparison/figures/linkage_distances.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This conforms with our expectation! If we look at the printout of the distances, we can see that the single linkage is slightly less for GENIA than for SciERC. However, the complete and average linkages are less for SciERC than they are for GENIA! This aligns with the idea that GENIA has a greater variety of words which \"distract\" the model from being able to perform well on PICKLE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dygiepp_fresh",
   "language": "python",
   "name": "dygiepp_fresh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
