{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DyGIE++: NER, RE, and EE with Coreference Resolution\n",
    "--------------------------------------\n",
    "[DyGIE++ paper](https://arxiv.org/pdf/1909.03546.pdf) <br>\n",
    "[DyGIE++ GitHub](https://github.com/dwadden/dygiepp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will implement the first 3 steps in KG construction, named entity recognition (NER), relation extraction (RE) and event extraction (EE) using the DyGIE++ pre-trained model on the [GENIA corpus](https://wayback.archive-it.org/org-350/20200626194727/https://orbit.nlm.nih.gov/browse-repository/dataset/human-annotated/83-genia-corpus). DyGIE++ performs these tasks simultaneously, using coreference resolution to enhance the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Formatting unlabeled data\n",
    "In order to apply a pre-trained model to unlabeled data, some formatting requirements must be met. From the [docs](https://github.com/dwadden/dygiepp/blob/master/doc/data.md): \n",
    "* \"In the case where your unlabeled data are stored as a directory of `.txt` files (one file per document), you can run `python scripts/data/new-dataset/format_new_dataset.py [input-directory] [output-file]` to format the documents into a `jsonl` file, with one line per document. If your dataset is scientific text, add the `--use-scispacy` flag to have SciSpacy do the tokenization.\"\n",
    "    * This is the most straightforward way to format new data. Since I am using abstracts which are downloaded from PubMed as a `.txt` file, and already have written a (rough) abstract extractor, this is fairly simple. \n",
    "* \"If you'd like to use a pretrained DyGIE++ model to make predictions on a new dataset, the `dataset` field in your new dataset must match the `dataset` that the original model was trained on; this indicates to the model which label namespace it should use for predictions.\"\n",
    "    * In this case, the `dataset` field for the unlabeled data should be `GENIA`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a directory of `.txt` files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a function to get abstracts from a PubMed search `.txt` results file (from NLP class project methods):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateAbstracts(data_path, abstract_num):\n",
    "    \"\"\"\n",
    "    Function to read a .txt file downloaded from PubMed and separate the text of the abstract from its metadata.\n",
    "    \n",
    "    parameters:\n",
    "        data_path, str: path to a .txt file with abstracts downloaded from PubMed\n",
    "        abstract_num, int: the number of abstracts in the file (from PubMed search interface)\n",
    "        \n",
    "    returns: \n",
    "        abstracts, list of str: list of the abstract plain text for all abstracts in data_path\n",
    "    \"\"\"\n",
    "    abstract_start_chars = [f'{x+1}. ' for x in range(abstract_num)]\n",
    "    abstract_start_re = '\\d+. '\n",
    "    \n",
    "    abstract_text = []\n",
    "    with open(data_path) as f:\n",
    "        \n",
    "        # Set up housekeeping variables\n",
    "        started_newline_count = False\n",
    "        newlines = 0\n",
    "        start_recording_abstract = False\n",
    "        current_abstract = ''\n",
    "        \n",
    "        # Iterate through lines in the file \n",
    "        for line in f:\n",
    "            # print(line)\n",
    "            ###################################\n",
    "            # 1. Find start of abstract section\n",
    "            ###################################\n",
    "            \n",
    "            if not started_newline_count and not start_recording_abstract: \n",
    "                # print('Looking for a start line...')\n",
    "                \n",
    "                # See if there's a number followed by a period in the line\n",
    "                match = re.search(abstract_start_re, line)\n",
    "                \n",
    "                if match is not None:\n",
    "                    \n",
    "                    # Check if it's the first thing in the line\n",
    "                    if match.start() == 0:\n",
    "                    \n",
    "                        # Check if it's in the list of abstract start characters\n",
    "                        if line[match.start():match.end()] in abstract_start_chars:\n",
    "                            \n",
    "                            # print('Found a start line!')\n",
    "                            # print(f'This line begins with {line[match.start():match.end()]}')\n",
    "                            started_newline_count = True \n",
    "                            \n",
    "                        \n",
    "            ########################################\n",
    "            # Count newlines until start of abstract\n",
    "            ########################################\n",
    "            \n",
    "            elif started_newline_count:\n",
    "                # print('Looking for the start of abstract text')\n",
    "                \n",
    "                # Check if this line is a newline \n",
    "                if line == '\\n':\n",
    "                    newlines += 1\n",
    "                    # print('This is a new line!')\n",
    "                    # print(f'Number of newlines including this one = {newlines}')\n",
    "                    \n",
    "                    if newlines == 4:\n",
    "                        # print(f'Found the start of an abstract! Begins with {line}')\n",
    "                        \n",
    "                        # If that was the fourth newline, indicate the next line starts the abstract\n",
    "                        start_recording_abstract = True \n",
    "                        \n",
    "                        # Reset the newlines counter variables\n",
    "                        started_newline_count = False\n",
    "                        newlines = 0\n",
    "                        \n",
    "                else: newlines += 0\n",
    "\n",
    "            #################\n",
    "            # Record abstract\n",
    "            #################\n",
    "            \n",
    "            elif start_recording_abstract:\n",
    "                \n",
    "                if line != '\\n':\n",
    "                    \n",
    "                    # Add this line to the current abstract \n",
    "                    current_abstract += line\n",
    "                    \n",
    "                elif line == '\\n':\n",
    "                    \n",
    "                    # Indicate that the abstract is over \n",
    "                    start_recording_abstract = False\n",
    "                    \n",
    "                    # Put the abstract in abstract list \n",
    "                    abstract_text.append(current_abstract)\n",
    "                    \n",
    "                    # Overwrite current_abstract\n",
    "                    current_abstract = ''\n",
    "                    \n",
    "                    \n",
    "    return abstract_text\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose abstracts to use. 50 Abstracts were selected from the PubMed search results for \"jasmonic acid arabidopsis\". Papers were manually selected as being \"molecular\" if they contained gene, protein, or pathway names in the title, or keywords like \"pathway\", \"signalling\" and \"crosstalk\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_num = 50\n",
    "data_path    = '../data/jasmonic_molec_abstract_50.txt'\n",
    "data_path    = os.path.abspath(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the file and extract abstracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example abstract:\n",
      "-------------------------------------------------\n",
      "Methyl jasmonate is a plant volatile that acts as an important cellular \n",
      "regulator mediating diverse developmental processes and defense responses. We \n",
      "have cloned the novel gene JMT encoding an S-adenosyl-l-methionine:jasmonic acid \n",
      "carboxyl methyltransferase (JMT) from Arabidopsis thaliana. Recombinant JMT \n",
      "protein expressed in Escherichia coli catalyzed the formation of methyl \n",
      "jasmonate from jasmonic acid with K(m) value of 38.5 microM. JMT RNA was not \n",
      "detected in young seedlings but was detected in rosettes, cauline leaves, and \n",
      "developing flowers. In addition, expression of the gene was induced both locally \n",
      "and systemically by wounding or methyl jasmonate treatment. This result suggests \n",
      "that JMT can perceive and respond to local and systemic signals generated by \n",
      "external stimuli, and that the signals may include methyl jasmonate itself. \n",
      "Transgenic Arabidopsis overexpressing JMT had a 3-fold elevated level of \n",
      "endogenous methyl jasmonate without altering jasmonic acid content. The \n",
      "transgenic plants exhibited constitutive expression of jasmonate-responsive \n",
      "genes, including VSP and PDF1.2. Furthermore, the transgenic plants showed \n",
      "enhanced level of resistance against the virulent fungus Botrytis cinerea. Thus, \n",
      "our data suggest that the jasmonic acid carboxyl methyltransferase is a key \n",
      "enzyme for jasmonate-regulated plant responses. Activation of JMT expression \n",
      "leads to production of methyl jasmonate that could act as an intracellular \n",
      "regulator, a diffusible intercellular signal transducer, and an airborne signal \n",
      "mediating intra- and interplant communications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abstract_texts = separateAbstracts(data_path, abstract_num)\n",
    "\n",
    "print('Example abstract:')\n",
    "print('-------------------------------------------------')\n",
    "print(abstract_texts[random.randint(0, abstract_num)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for extraction exceptions (see explanation in NLP project methods development notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 abstracts were lost to foreign language formatting edge case\n"
     ]
    }
   ],
   "source": [
    "# Drop any texts that match the author info regex\n",
    "author_info_re = 'Author information:'\n",
    "\n",
    "abstract_texts_clean = [x for x in abstract_texts if re.match(author_info_re, x) is None]\n",
    "\n",
    "print(f'{len(abstract_texts) - len(abstract_texts_clean)} abstracts were lost to foreign language formatting edge case')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something weird happened in abstract 0 - noticed it when randomly spot checking the output files after my first pass at this implementation. Text of the abstract section for the anomaly: \n",
    "```\n",
    "1. Plant Cell Physiol. 2018 Jan 1;59(1):8-16. doi: 10.1093/pcp/pcx181.\n",
    "\n",
    "Salicylic Acid and Jasmonic Acid Pathways are Activated in Spatially Different \n",
    "Domains Around the Infection Site During Effector-Triggered Immunity in \n",
    "Arabidopsis thaliana.\n",
    "\n",
    "Betsuyaku S(1), Katou S(2), Takebayashi Y(3), Sakakibara H(3), Nomura N(1), \n",
    "Fukuda H(4).\n",
    "\n",
    "Author information:\n",
    "(1)Faculty of Life and Environmental Sciences, University of Tsukuba, 1-1-1 \n",
    "Tennodai, Tsukuba, Ibarakim 305-8577 Japan.\n",
    "(2)Institute of Agriculture, Academic Assembly, Shinshu University, 8304, \n",
    "Minamiminowa, Nagano, 399-4598 Japan.\n",
    "(3)Plant Productivity Systems Research Group, RIKEN Center for Sustainable \n",
    "Resource Science, 1-7-22, Suehiro, Tsurumi-ku, Yokohama, 230-0045 Japan.\n",
    "(4)Department of Biological Sciences, Graduate School of Science, The University \n",
    "of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-0033, Japan.\n",
    "\n",
    "Erratum in\n",
    "    Plant Cell Physiol. 2018 Feb 1;59(2):439.\n",
    "\n",
    "Comment in\n",
    "    Plant Cell Physiol. 2018 Jan 1;59(1):3-4.\n",
    "\n",
    "The innate immune response is, in the first place, elicited at the site of \n",
    "infection. Thus, the host response can be different among the infected cells and \n",
    "the cells surrounding them. Effector-triggered immunity (ETI), a form of innate \n",
    "immunity in plants, is triggered by specific recognition between pathogen \n",
    "effectors and their corresponding plant cytosolic immune receptors, resulting in \n",
    "rapid localized cell death known as hypersensitive response (HR). HR cell death \n",
    "is usually limited to a few cells at the infection site, and is surrounded by a \n",
    "few layers of cells massively expressing defense genes such as \n",
    "Pathogenesis-Related Gene 1 (PR1). This virtually concentric pattern of the \n",
    "cellular responses in ETI is proposed to be regulated by a concentration \n",
    "gradient of salicylic acid (SA), a phytohormone accumulated around the infection \n",
    "site. Recent studies demonstrated that jasmonic acid (JA), another phytohormone \n",
    "known to be mutually antagonistic to SA in many cases, is also accumulated in \n",
    "and required for ETI, suggesting that ETI is a unique case. However, the \n",
    "molecular basis for this uniqueness remained largely to be solved. Here, we \n",
    "found that, using intravital time-lapse imaging, the JA signaling pathway is \n",
    "activated in the cells surrounding the central SA-active cells around the \n",
    "infection sites in Arabidopsis thaliana. This distinct spatial organization \n",
    "explains how these two phythormone pathways in a mutually antagonistic \n",
    "relationship can be activated simultaneously during ETI. Our results \n",
    "re-emphasize that the spatial consideration is a key strategy to gain \n",
    "mechanistic insights into the apparently complex signaling cross-talk in \n",
    "immunity.\n",
    "\n",
    "© The Author 2017. Published by Oxford University Press on behalf of Japanese \n",
    "Society of Plant Physiologists.\n",
    "\n",
    "DOI: 10.1093/pcp/pcx181\n",
    "PMCID: PMC6012717\n",
    "PMID: 29177423 [Indexed for MEDLINE]\n",
    "```\n",
    "\n",
    "The \"Erratum\" and \"Comment\" lines added extra newlines, and the \"Erratum\" for counted as the abstract because it came after the 4th newline. At the moment I'm not super concerned about losing a few abstracts here and there to this kind of edge case, because the convenience/memory of this solution is better in the short term, but in the long run probably better to pull XML files with the script I've previously written and get the abstract from the structured text. In order to deal with edge cases here, however, I can just flag abstracts that have fewer than 5 lines OR start with \"Author Information:\" (the Author Information section can have more than 5 lines) so I don't have to check all of them individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for further edge cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 problem abstracts. These are:\n",
      "----------------------------------------\n",
      "\n",
      "Erratum in\n",
      "    Plant Cell Physiol. 2018 Feb 1;59(2):439.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Comment in\n",
      "    Nat Plants. 2018 May;4(5):240.\n",
      "    Plant Cell. 2018 May;30(5):948-949.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "© The Author 2016. Published by Oxford University Press on behalf of the Society \n",
      "for Experimental Biology.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Comment in\n",
      "    Sci China Life Sci. 2015 Mar;58(3):311-2.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Comment in\n",
      "    New Phytol. 2017 Sep;215(4):1291-1294.\n",
      "\n",
      "\n",
      "The indices of these in the abstracts list are [0, 8, 11, 25, 48]\n"
     ]
    }
   ],
   "source": [
    "problem_abstracts = [x for x in abstract_texts_clean if x.count('\\n') < 4]\n",
    "\n",
    "print(f'There are {len(problem_abstracts)} problem abstracts. These are:')\n",
    "for i in range(len(problem_abstracts)):\n",
    "    print('----------------------------------------\\n')\n",
    "    print(problem_abstracts[i])\n",
    "    \n",
    "problem_indices = [abstract_texts_clean.index(x) for x in problem_abstracts]\n",
    "print(f'\\nThe indices of these in the abstracts list are {problem_indices}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of abstracts is 44\n"
     ]
    }
   ],
   "source": [
    "# Drop the edge cases\n",
    "abstracts_to_write = [x for x in abstract_texts_clean if abstract_texts_clean.index(x) not in problem_indices]\n",
    "\n",
    "print(f'Final number of abstracts is {len(abstracts_to_write)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "### TODO: \n",
    "Turns out the `PubMed` format has a much more well-defined field for abstract that would circumvent the need for all the above edge case detection and elimination. Go back and write code to utilize that format!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "Write the abstracts to `.txt` files, one per abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/dygiepp_50_molec/'\n",
    "data_dir = os.path.abspath(data_dir)\n",
    "\n",
    "for i, abstract in enumerate(abstracts_to_write):\n",
    "        with open(f'{data_dir}/abstract{i}', 'w') as f:\n",
    "            f.write(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the dygiepp command line to format the documents into a `jsonl` file. In the directory `~/projects/knowledge-graph/dygiepp`, run:\n",
    "```\n",
    "python scripts/new-dataset/format_new_dataset.py ../data/dygiepp_50_molec/ ../data/dygiepp_50_molec/50_molec.jsonl genia --use-scispacy\n",
    "```\n",
    "**Note:** You must create an empty file with the correct name before running this line, or else nothing will be written anywhere.\n",
    "* **DONE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "## 1. Making predictions with pre-trained GENIA model\n",
    "*From the docs:*\n",
    "----------------------------\n",
    "To make predictions on a new, unlabeled dataset:\n",
    "\n",
    "1. Download the pretrained model that most closely matches your text domain.\n",
    "2. Make sure that the dataset field for your new dataset matches the label namespaces for the pretrained model. See here for more on label namespaces. To view the available label namespaces for a pretrained model, use print_label_namespaces.py.\n",
    "3. Make predictions the same way as with the existing datasets:\n",
    "```\n",
    "allennlp predict pretrained/[name-of-pretrained-model].tar.gz \\\n",
    "    [input-path] \\\n",
    "    --predictor dygie \\\n",
    "    --include-package dygie \\\n",
    "    --use-dataset-reader \\\n",
    "    --output-file [output-path] \\\n",
    "    --cuda-device [cuda-device]\n",
    "```\n",
    "A couple tricks to make things run smoothly:\n",
    "\n",
    "1. If you're predicting on a big dataset, you probably want to load it lazily rather than loading the whole thing in before predicting. To accomplish this, add the following flag to the above command:\n",
    "\n",
    "```\n",
    "--overrides \"{'dataset_reader' +: {'lazy': true}}\"\n",
    "```\n",
    "\n",
    "2. If the model runs out of GPU memory on a given prediction, it will warn you and continue with the next example rather than stopping entirely. This is less annoying than the alternative. Examples for which predictions failed will still be written to the specified `jsonl` output, but they will have an additional field `{\"_FAILED_PREDICTION\": true}` indicating that the model ran out of memory on this example.\n",
    "3. The `dataset` field in the dataset to be predicted must match one of the datasets on which the model was trained; otherwise, the model won't know which labels to apply to the predicted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command line to make predictions on the mini-dataset formatted above:\n",
    "\n",
    "```\n",
    "allennlp predict ~/Downloads/genia.tar.gz \\\n",
    "~/projects/knowledge-graph/data/dygiepp_50_molec/50_molec.jsonl \\\n",
    "--predictor dygie \\\n",
    "--include-package dygie \\\n",
    "--use-dataset-reader \\\n",
    "--output-file ~/projects/knowledge-graph/data/dygiepp_50_molec/50_molec_predictions_genia.jsonl \\\n",
    "--cuda-device 0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dygiepp]",
   "language": "python",
   "name": "conda-env-dygiepp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
