{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DyGIE++: NER & RE with Coreference Resolution\n",
    "--------------------------------------\n",
    "[DyGIE++ paper](https://arxiv.org/pdf/1909.03546.pdf) <br>\n",
    "[DyGIE++ GitHub](https://github.com/dwadden/dygiepp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, named entity recognition (NER) & relation extraction (RE) will be implemented using pre-trained models with DyGIE++. DyGIE++ performs these tasks simultaneously, using coreference resolution to enhance the performance of the model.\n",
    "<br><br>\n",
    "In the long term, new data to train a model is being annotated in the same style as GENIA. However, GENIA only has an annotation set for static relations, not causal relations, and the relations have been completely excluded from the DyGIE++ implementation for the GENIA pre-trained model. For this reason, GENIA has been excluded from this notebook, and ACE05 and SciERC will be used.\n",
    "<br><br>\n",
    "For the command lines in this notebook, the following directory structure is assumed:\n",
    "```\n",
    "root_dir\n",
    "    |\n",
    "    |── knowledge-graph/\n",
    "    |\n",
    "    └── dygiepp/ \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "## 0. Formatting unlabeled data\n",
    "--------------------------------\n",
    "In order to apply a pre-trained model to unlabeled data, some formatting requirements must be met. From the [docs](https://github.com/dwadden/dygiepp/blob/master/doc/data.md): \n",
    "* In the case where your unlabeled data are stored as a directory of `.txt` files (one file per document), you can run `python scripts/data/new-dataset/format_new_dataset.py [input-directory] [output-file]` to format the documents into a `jsonl` file, with one line per document. If your dataset is scientific text, add the `--use-scispacy` flag to have SciSpacy do the tokenization.\n",
    "    \n",
    "* If you'd like to use a pretrained DyGIE++ model to make predictions on a new dataset, the `dataset` field in your new dataset must match the `dataset` that the original model was trained on; this indicates to the model which label namespace it should use for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a directory of `.txt` files\n",
    "This is done with a script on the command line, `knowledge-graph/data_retrieval/abstracts_only/getAbstracts.py`. The data to be extracted come from downloading PubMed format files for all search results for the two searches \"jasmonic acid\" and \"gibberellic acid\", and is found in the directory `knowledge-graph/data/first_manuscript_data/raw_abstracts/`. The command lines used to extract the data (while in the `knowledge-graph/data_retreival/abstracts_only` directory) are:\n",
    "\n",
    "```\n",
    "python getAbstracts.py -abstracts_txt ../../data/first_manuscript_data/pubmed_files/pubmed-jasmonicac-set-ALL-RESULTS.txt -dest_dir ../../data/first_manuscript_data/raw_abstracts/\n",
    "\n",
    "python getAbstracts.py -abstracts_txt ../../data/first_manuscript_data/pubmed_files/pubmed-gibberelli-set-ALL-RESULTS.txt -dest_dir ../../data/first_manuscript_data/raw_abstracts/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose specific docs with clustering pipeline\n",
    "\n",
    "This is done with the scripts in `knowledge_graphs/data_retreival/doc_clustering`. Command line run from `knowledge-graph/data_retreival/doc_clustering/`:\n",
    "\n",
    "```\n",
    "python doc_clustering.py -data ../../data/first_manuscript_data/raw_abstracts/ -num_abstracts 8000 -out_loc ../../data/first_manuscript_data/clustering_pipeline_output/ -new_dir_name JA+GA_chosen_abstracts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format for input to DyGIE++ \n",
    "\n",
    "Now we can use the dygiepp command line to format the documents into a `jsonl` file. We have to do this separately for each of SciERC and ACE05 (SciERC lightweight and SciERC. In the directory `dygiepp/`, run:\n",
    "\n",
    "#### SciERC\n",
    "\n",
    "```\n",
    "# Create the file for the output \n",
    "vim ../knowledge-graph/data/first_manuscript_data/dygiepp/prepped_data/dygiepp_formatted_data_SciERC.jsonl\n",
    "\n",
    "python scripts/new-dataset/format_new_dataset.py ../knowledge-graph/data/first_manuscript_data/clustering_pipeline_output/JA+GA_chosen_abstracts/ ../knowledge-graph/data/first_manuscript_data/dygiepp/prepped_data/dygiepp_formatted_data_SciERC.jsonl scierc\n",
    "```\n",
    "#### ACE05\n",
    "\n",
    "```\n",
    "# Create the file for the output \n",
    "vim ../knowledge-graph/data/first_manuscript_data/dygiepp/prepped_data/dygiepp_formatted_data_ACE05.jsonl\n",
    "\n",
    "python scripts/new-dataset/format_new_dataset.py ../knowledge-graph/data/first_manuscript_data/clustering_pipeline_output/JA+GA_chosen_abstracts/ ../knowledge-graph/data/first_manuscript_data/dygiepp/prepped_data/dygiepp_formatted_data_ACE05.jsonl ace05\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "## 1. Making predictions with pre-trained models\n",
    "----------------------------\n",
    "From the docs:<br>\n",
    "To make predictions on a new, unlabeled dataset:\n",
    "\n",
    "1. Download the pretrained model that most closely matches your text domain.\n",
    "2. Make sure that the dataset field for your new dataset matches the label namespaces for the pretrained model. See here for more on label namespaces. To view the available label namespaces for a pretrained model, use print_label_namespaces.py.\n",
    "3. Make predictions the same way as with the existing datasets:\n",
    "```\n",
    "allennlp predict pretrained/[name-of-pretrained-model].tar.gz \\\n",
    "    [input-path] \\\n",
    "    --predictor dygie \\\n",
    "    --include-package dygie \\\n",
    "    --use-dataset-reader \\\n",
    "    --output-file [output-path] \\\n",
    "    --cuda-device [cuda-device]\n",
    "```\n",
    "A couple tricks to make things run smoothly:\n",
    "\n",
    "1. If you're predicting on a big dataset, you probably want to load it lazily rather than loading the whole thing in before predicting. To accomplish this, add the following flag to the above command:\n",
    "\n",
    "```\n",
    "--overrides \"{'dataset_reader' +: {'lazy': true}}\"\n",
    "```\n",
    "\n",
    "2. If the model runs out of GPU memory on a given prediction, it will warn you and continue with the next example rather than stopping entirely. This is less annoying than the alternative. Examples for which predictions failed will still be written to the specified `jsonl` output, but they will have an additional field `{\"_FAILED_PREDICTION\": true}` indicating that the model ran out of memory on this example.\n",
    "3. The `dataset` field in the dataset to be predicted must match one of the datasets on which the model was trained; otherwise, the model won't know which labels to apply to the predicted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models were run using the following job scripts (pretrained models were downloaded by running `bash scripts/pretrained/get_dygiepp_pretrained.sh` from the `dygiepp/` directory, which puts them in a directory called `dygiepp/pretrained/`. Just an fyi, this takes forever). Scripts are found in `knowledge-graph/job_scripts/`\n",
    "\n",
    "### Full models (with coreference)\n",
    "#### SciERC\n",
    "\n",
    "```\n",
    "#!/bin/bash --login\n",
    "######################### Resources #################################\n",
    "\n",
    "#SBATCH --time=03:59:59 \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --gpus=1\n",
    "#SBATCH --job-name scierc_apply\n",
    "\n",
    "\n",
    "######################## Command Lines for Job ########################\n",
    "\n",
    "module load CUDA/9.2.88\n",
    "\n",
    "conda activate kg\n",
    "cd ~/Shiu_lab/dygiepp\n",
    "\n",
    "allennlp predict pretrained/scierc.tar.gz ../knowledge-graph/data/first_manuscript_data/dygiepp/prepped_data/dygiepp_formatted_data_SciERC.jsonl --predictor dygie --include-package dygie --use-dataset-reader --output-file ../knowledge-graph/data/first_manuscript_data/dygiepp/pretrained_output/SciERC_predictions.jsonl --cuda-device 0 --overrides \"{'dataset_reader' +: {'lazy': true}}\"                  \n",
    "```\n",
    "\n",
    "#### ACE05\n",
    "\n",
    "\n",
    "```\n",
    "#!/bin/bash --login\n",
    "######################### Resources #################################\n",
    "\n",
    "#SBATCH --time=03:59:59 \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --gpus=1\n",
    "#SBATCH --job-name ace05_apply\n",
    "\n",
    "\n",
    "######################## Command Lines for Job ########################\n",
    "\n",
    "module load CUDA/9.2.88\n",
    "\n",
    "conda activate kg\n",
    "cd ~/Shiu_lab/dygiepp\n",
    "\n",
    "allennlp predict pretrained/ace05-relations.tar.gz ../knowledge-graph/data/first_manuscript_data/dygiepp/prepped_data/dygiepp_formatted_data_ACE05.jsonl --predictor dygie --include-package dygie --use-dataset-reader --output-file ../knowledge-graph/data/first_manuscript_data/dygiepp/pretrained_output/ACE05_predictions.jsonl --cuda-device 0 --overrides \"{'dataset_reader' +: {'lazy': true}}\"                  \n",
    "```\n",
    "\n",
    "**NOTE:** The path where data is stored  has been changed since this was run -- data were moved to a new directory called `withCoref` within the `dygiepp/pretrained_output` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------\n",
    "### Lightweight models (without coreference)\n",
    "\n",
    "SciERC has a lightweight model in which coreferences are ignored. This is necessary for the manuscript, because my model trained from scratch won't have coreferences. The ACE05 model available seems to have no coreferences, and so should be in this sectionl, but waiting for confirmation from Dave before changing where the prediction data & graphs are stored.\n",
    "\n",
    "#### SciERC\n",
    "\n",
    "```\n",
    "#!/bin/bash --login\n",
    "######################### Resources #################################\n",
    "\n",
    "#SBATCH --time=03:59:59 \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --gpus=1\n",
    "#SBATCH --job-name scierc_apply\n",
    "\n",
    "\n",
    "######################## Command Lines for Job ########################\n",
    "\n",
    "module load CUDA/9.2.88\n",
    "\n",
    "conda activate kg\n",
    "cd ~/Shiu_lab/dygiepp\n",
    "\n",
    "allennlp predict pretrained/scierc-lightweight.tar.gz ../knowledge-graph/data/first_manuscript_data/dygiepp/prepped_data/dygiepp_formatted_data_SciERC.jsonl --predictor dygie --include-package dygie --use-dataset-reader --output-file ../knowledge-graph/data/first_manuscript_data/dygiepp/pretrained_output/lightweight/SciERC_predictions.jsonl --cuda-device 0 --overrides \"{'dataset_reader' +: {'lazy': true}}\"                  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## 2. Looking at the predictions \n",
    "----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's look at how many docs have failed predictions, or missing ones, comapred to the total number of docs in the prepared data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input docs for each model:\n",
      "-------------------------------------\n",
      "SciERC: 8000   ACE05: 8000\n"
     ]
    }
   ],
   "source": [
    "# Count prepped data\n",
    "scierc_input = 0\n",
    "with jsonlines.open('../data/first_manuscript_data/dygiepp/prepped_data/dygiepp_formatted_data_SciERC.jsonl') as reader:\n",
    "    for obj in reader:\n",
    "        scierc_input += 1\n",
    "        \n",
    "ace05_input = 0\n",
    "with jsonlines.open('../data/first_manuscript_data/dygiepp/prepped_data/dygiepp_formatted_data_ACE05.jsonl') as reader:\n",
    "    for obj in reader:\n",
    "        ace05_input += 1\n",
    "        \n",
    "print('Number of input docs for each model:')\n",
    "print('-------------------------------------')\n",
    "print(f'SciERC: {scierc_input}   ACE05: {ace05_input}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for failed docs\n",
    "scierc_output_total = 0\n",
    "scierc_failed = 0\n",
    "with jsonlines.open('../data/first_manuscript_data/dygiepp/pretrained_output/SciERC_predictions.jsonl') as reader:\n",
    "    for obj in reader:\n",
    "        scierc_output_total += 1\n",
    "        if \"_FAILED_PREDICTION\" in obj.keys():\n",
    "            if obj[\"_FAILED_PREDICTION\"]:\n",
    "                scierc_failed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ace05_output_total = 0\n",
    "ace05_failed = 0\n",
    "with jsonlines.open('../data/first_manuscript_data/dygiepp/pretrained_output/ACE05_predictions.jsonl') as reader:\n",
    "    for obj in reader:\n",
    "        ace05_output_total += 1\n",
    "        if \"_FAILED_PREDICTION\" in obj.keys():\n",
    "            if obj[\"_FAILED_PREDICTION\"]:\n",
    "                ace05_failed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of output docs for each model:\n",
      "--------------------------------------\n",
      "SciERC: 8000      ACE05: 8000\n",
      "\n",
      "Number of failed predictions for SciERC: 0\n",
      "Number of failed predictions for ACE05: 0\n"
     ]
    }
   ],
   "source": [
    "print('Number of output docs for each model:')\n",
    "print('--------------------------------------')\n",
    "print(f'SciERC: {scierc_output_total}      ACE05: {ace05_output_total}')\n",
    "print(f'\\nNumber of failed predictions for SciERC: {scierc_failed}')\n",
    "print(f'Number of failed predictions for ACE05: {ace05_failed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the SciERC output further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data \n",
    "scierc_output = []\n",
    "with jsonlines.open('../data/first_manuscript_data/dygiepp/pretrained_output/SciERC_predictions.jsonl') as reader:\n",
    "    for obj in reader:\n",
    "        scierc_output.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentences', 'predicted_relations', 'predicted_ner', 'dataset', 'doc_key', 'predicted_clusters'}\n"
     ]
    }
   ],
   "source": [
    "# Look at what keys are in the output dicts \n",
    "output_keys = set([key for key in obj.keys() for obj in scierc_output])\n",
    "print(output_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A rapid , simple , and stringent protocol for the detection and quantitation of jasmonic acid ( JA ) is designed using high - performance thin - layer chromatography . Acidified culture filtrate of Lasiodiplodia theobromae is extracted with an equal volume of ethyl acetate and spotted on silica gel 60 F(254 ) foil using Linomat-5 spray - on applicator . Standard JA is also spotted either internally or adjacent to the sample , and the foils are developed with isopropanol - ammonia - water [ 10:1:1 ( v / v ) ] as the mobile phase . A quantitative estimation of the separated JA is performed by measuring the absorbance at 295 nm in the reflective mode . The sensitivity of the method is improved by adding internal standard to obtain a detection limit of 1 microg . The limit of quantitation is found to be 80 microg with this method . The method is shown to have selectivity , accuracy , precision , and high sample throughput , making it useful for the routine analysis of JA in basic science and perfumery industries . \n"
     ]
    }
   ],
   "source": [
    "# Look at an example doc \n",
    "\n",
    "# Reconstruct the abstract from the sentences entry in the dict\n",
    "abstract = ''\n",
    "for sentence in scierc_output[0]['sentences']:\n",
    "    sentence = ' '.join(sentence) # Adds spaces before punctuation as well\n",
    "    abstract += f'{sentence} '\n",
    "    \n",
    "print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['protocol', 'JA', 'Lasiodiplodia theobromae', 'ethyl acetate', 'Linomat-5 spray - on applicator', 'JA', 'foils', 'isopropanol -', '-', 'mobile phase', 'quantitative estimation of the separated JA', 'reflective mode', 'sensitivity', 'method', 'internal standard', 'detection limit', 'limit of quantitation', 'method', 'method', 'selectivity', 'accuracy', 'precision', 'sample throughput', 'it', 'JA']\n"
     ]
    }
   ],
   "source": [
    "# Look at predicted entities\n",
    "tokenized_doc = []\n",
    "for sentence in scierc_output[0]['sentences']:\n",
    "    for word in sentence:\n",
    "        tokenized_doc.append(word)\n",
    "\n",
    "entity_list = []\n",
    "for sent_ent_list in scierc_output[0]['predicted_ner']:\n",
    "    entities = []\n",
    "    for ent_list in sent_ent_list:\n",
    "        ent = \" \".join(tokenized_doc[ent_list[0]:ent_list[1]+1])\n",
    "        entities.append(ent)\n",
    "    entity_list += entities\n",
    "    \n",
    "print(entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('high - performance thin - layer chromatography', 'USED-FOR', 'protocol'), ('sensitivity', 'EVALUATE-FOR', 'method'), ('internal standard', 'USED-FOR', 'method'), ('selectivity', 'EVALUATE-FOR', 'method'), ('selectivity', 'CONJUNCTION', 'accuracy'), ('selectivity', 'CONJUNCTION', 'precision'), ('accuracy', 'EVALUATE-FOR', 'method'), ('precision', 'EVALUATE-FOR', 'method')]\n"
     ]
    }
   ],
   "source": [
    "# Look at predicted relations\n",
    "relations_list = []\n",
    "for sent_rel_list in scierc_output[0]['predicted_relations']:\n",
    "    rels = []\n",
    "    for rel_list in sent_rel_list:\n",
    "        rel = (\" \".join(tokenized_doc[rel_list[0]:rel_list[1]+1]), rel_list[4],\n",
    "               \" \".join(tokenized_doc[rel_list[2]:rel_list[3]+1]))\n",
    "        rels.append(rel)\n",
    "    relations_list += rels\n",
    "\n",
    "print(relations_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the ACE05 data for the same abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data \n",
    "ace05_output = []\n",
    "with jsonlines.open('../data/first_manuscript_data/dygiepp/pretrained_output/ACE05_predictions.jsonl') as reader:\n",
    "    for obj in reader:\n",
    "        ace05_output.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentences', 'predicted_relations', 'predicted_ner', 'dataset', 'doc_key'}\n"
     ]
    }
   ],
   "source": [
    "# Look at what keys are in the output dicts \n",
    "output_keys_ace05 = set([key for key in obj.keys() for obj in scierc_output])\n",
    "print(output_keys_ace05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A rapid , simple , and stringent protocol for the detection and quantitation of jasmonic acid ( JA ) is designed using high - performance thin - layer chromatography . Acidified culture filtrate of Lasiodiplodia theobromae is extracted with an equal volume of ethyl acetate and spotted on silica gel 60 F(254 ) foil using Linomat-5 spray - on applicator . Standard JA is also spotted either internally or adjacent to the sample , and the foils are developed with isopropanol - ammonia - water [ 10:1:1 ( v / v ) ] as the mobile phase . A quantitative estimation of the separated JA is performed by measuring the absorbance at 295 nm in the reflective mode . The sensitivity of the method is improved by adding internal standard to obtain a detection limit of 1 microg . The limit of quantitation is found to be 80 microg with this method . The method is shown to have selectivity , accuracy , precision , and high sample throughput , making it useful for the routine analysis of JA in basic science and perfumery industries . \n"
     ]
    }
   ],
   "source": [
    "# Look at an example doc \n",
    "\n",
    "# Reconstruct the abstract from the sentences entry in the dict\n",
    "abstract_ace05 = ''\n",
    "for sentence in ace05_output[0]['sentences']:\n",
    "    sentence = ' '.join(sentence) # Adds spaces before punctuation as well\n",
    "    abstract_ace05 += f'{sentence} '\n",
    "    \n",
    "print(abstract_ace05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['industries']\n"
     ]
    }
   ],
   "source": [
    "# Look at predicted entities\n",
    "tokenized_doc_ace05 = []\n",
    "for sentence in ace05_output[0]['sentences']:\n",
    "    for word in sentence:\n",
    "        tokenized_doc_ace05.append(word)\n",
    "\n",
    "entity_list_ace05 = []\n",
    "for sent_ent_list in ace05_output[0]['predicted_ner']:\n",
    "    entities = []\n",
    "    for ent_list in sent_ent_list:\n",
    "        ent = \" \".join(tokenized_doc_ace05[ent_list[0]:ent_list[1]+1])\n",
    "        entities.append(ent)\n",
    "    entity_list_ace05 += entities\n",
    "    \n",
    "print(entity_list_ace05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Look at predicted relations\n",
    "relations_list_ace05 = []\n",
    "for sent_rel_list in ace05_output[0]['predicted_relations']:\n",
    "    rels = []\n",
    "    for rel_list in sent_rel_list:\n",
    "        rel = (\" \".join(tokenized_doc_ace05[rel_list[0]:rel_list[1]+1]), rel_list[4], \n",
    "               \" \".join(tokenized_doc_ace05[rel_list[2]:rel_list[3]+1]))\n",
    "        rels.append(rel)\n",
    "    relations_list += rels\n",
    "\n",
    "print(relations_list_ace05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg",
   "language": "python",
   "name": "kg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
